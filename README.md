
## ðŸ§ª GPU:
1.  faster-whisper        = ~1940 GB
2.  tts                   = ~620  GB
3.  Qwen/Qwen3-0.6B-FP8   = ~1360 GB
4.  Qwen.Qwen3-0.6B       = ~1760 GB
5.  Qwen/Qwen3-1.7B       = ~4240 GB
6.  Qwen/Qwen2.5-0.5B     = ~1200 GB
7.  Total GPU             = ~4450 GB (faster-whisper+tts+qwen3-0.6B)
8.  Total GPU             = ~6800 GB (faster-whisper+tts+qwen3-1.7B)
9.  Total GPU             = ~4200 GB (faster-whisper+tts+Qwen2.5-0.5B)
10. Total GPU             = ~4266 GB (faster-whisper+tts+Qwen3-0.6B-FP8)
11. Total GPU             = ~4254 GB (faster-whisper-medium+tts+Qwen3-0.6B-FP8)

## ðŸ§ª Qwen/Qwen3-1.7B/0.6B:
1. prompt: 32K
2. Trained on same data sets (sizes)